// ============================================================
// Token Benchmark â€” Full vs. Minimal MCP Server Comparison
// Proves the value of minimal-cover servers with hard numbers.
// ============================================================

import { encode } from "gpt-tokenizer";
import { ApiEndpoint, ENDPOINTS } from "../registry/endpoints";

export interface BenchmarkResult {
    fullToolCount: number;
    minimalToolCount: number;
    fullTokens: number;
    minimalTokens: number;
    savedTokens: number;
    savingsPercent: number;
    fullEstCost: number;
    minimalEstCost: number;
    savedCost: number;
    details: {
        endpointId: string;
        tokens: number;
    }[];
}

// GPT-4o pricing: $2.50 per 1M input tokens
const COST_PER_TOKEN = 2.5 / 1_000_000;

/**
 * Generate the MCP tool definition text for a single endpoint
 * (what gets sent into the LLM context window)
 */
function toolDefinitionText(ep: ApiEndpoint): string {
    const params = ep.parameters.map((p) => {
        return `    - ${p.name} (${p.type}${p.required ? ", required" : ", optional"}): ${p.description}`;
    });

    return [
        `Tool: ${ep.id}`,
        `  Description: ${ep.description}`,
        `  Method: ${ep.method} ${ep.path}`,
        `  RequiresAuth: ${ep.requiresAuth}`,
        params.length > 0 ? `  Parameters:\n${params.join("\n")}` : `  Parameters: none`,
    ].join("\n");
}

/**
 * Count tokens for a set of tool definitions
 */
function countTokens(endpoints: ApiEndpoint[]): { totalTokens: number; perEndpoint: { endpointId: string; tokens: number }[] } {
    const perEndpoint = endpoints.map((ep) => {
        const text = toolDefinitionText(ep);
        const tokens = encode(text).length;
        return { endpointId: ep.id, tokens };
    });

    const totalTokens = perEndpoint.reduce((sum, e) => sum + e.tokens, 0);
    return { totalTokens, perEndpoint };
}

/**
 * Run benchmark comparing full server vs. minimal server
 */
export function runBenchmark(selectedEndpoints: ApiEndpoint[]): BenchmarkResult {
    const full = countTokens(ENDPOINTS);
    const minimal = countTokens(selectedEndpoints);

    const savedTokens = full.totalTokens - minimal.totalTokens;
    const savingsPercent = Math.round((savedTokens / full.totalTokens) * 100);

    return {
        fullToolCount: ENDPOINTS.length,
        minimalToolCount: selectedEndpoints.length,
        fullTokens: full.totalTokens,
        minimalTokens: minimal.totalTokens,
        savedTokens,
        savingsPercent,
        fullEstCost: full.totalTokens * COST_PER_TOKEN,
        minimalEstCost: minimal.totalTokens * COST_PER_TOKEN,
        savedCost: savedTokens * COST_PER_TOKEN,
        details: minimal.perEndpoint,
    };
}

/**
 * Format benchmark result as a nice table
 */
export function formatBenchmarkTable(result: BenchmarkResult, profileName: string): string {
    const lines: string[] = [];

    lines.push("");
    lines.push("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    lines.push("â•‘        ğŸ”¬ MCP SERVER TOKEN BENCHMARK â€” MINIMAL vs FULL         â•‘");
    lines.push("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£");
    lines.push(`â•‘  Profile: ${profileName.padEnd(53)}â•‘`);
    lines.push("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£");
    lines.push("â•‘ Metric                   â•‘ Full      â•‘ Minimal   â•‘ Savings      â•‘");
    lines.push("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£");
    lines.push(
        `â•‘ Tool definitions         â•‘ ${String(result.fullToolCount).padStart(7)}   â•‘ ${String(result.minimalToolCount).padStart(7)}   â•‘ ${String(result.fullToolCount - result.minimalToolCount).padStart(5)} fewer   â•‘`
    );
    lines.push(
        `â•‘ Token count              â•‘ ${String(result.fullTokens).padStart(7).replace(/\B(?=(\d{3})+(?!\d))/g, ",")}   â•‘ ${String(result.minimalTokens).padStart(7).replace(/\B(?=(\d{3})+(?!\d))/g, ",")}   â•‘ ${String(result.savingsPercent).padStart(4)}% saved  â•‘`
    );
    lines.push(
        `â•‘ Est. cost/call (GPT-4o)  â•‘ $${result.fullEstCost.toFixed(5).padStart(7)}   â•‘ $${result.minimalEstCost.toFixed(5).padStart(7)}   â•‘ $${result.savedCost.toFixed(5).padStart(7)}     â•‘`
    );
    lines.push("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");

    lines.push("");
    lines.push("ğŸ“Š Per-tool token breakdown (minimal server):");
    lines.push("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”");
    lines.push("â”‚ Tool                   â”‚ Tokens â”‚");
    lines.push("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤");
    for (const d of result.details) {
        lines.push(`â”‚ ${d.endpointId.padEnd(22)} â”‚ ${String(d.tokens).padStart(6)} â”‚`);
    }
    lines.push("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜");

    lines.push("");
    lines.push(`ğŸ’¡ Bottom line: Using a minimal "${profileName}" server saves ${result.savingsPercent}% of tokens`);
    lines.push(`   compared to loading all ${result.fullToolCount} tools. That's ${result.savedTokens} fewer tokens`);
    lines.push(`   in the LLM context window per request.`);
    lines.push("");

    return lines.join("\n");
}
